# -*- coding: utf-8 -*-
"""Analyze clusters.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uZwfjJbC7nLK75_1_xImUKwMOixaEwLp

# Affected elements
"""

#%%
import pandas as pd

data = pd.read_csv('cluster_stars_with_features.csv')
data['repo_original'] = data['original_path'].str.split('$').str[0] + '/' + data['original_path'].str.split('$').str[1]
data['repo_duplicate'] = data['duplicate_path'].str.split('$').str[0] + '/' + data['duplicate_path'].str.split('$').str[1]
data['inter'] = data['repo_original'] != data['repo_duplicate']
data

#%%
all_features = ["ADD-EAnnotation", "ADD-EAttribute", "ADD-EClass", "ADD-EDataType", "ADD-EEnum", "ADD-EEnumLiteral", "ADD-EGenericType",
				"ADD-EOperation", "ADD-EPackage", "ADD-EReference", "ADD-ETypeParameter", "ADD-ResourceAttachment.EDataType",
				"ADD-ResourceAttachment.EPackage", "CHANGE-EAnnotation", "CHANGE-EAttribute", "CHANGE-EClass", "CHANGE-EDataType", "CHANGE-EEnum",
				"CHANGE-EEnumLiteral", "CHANGE-EGenericType", "CHANGE-EOperation", "CHANGE-EPackage", "CHANGE-EParameter", "CHANGE-EReference",
				"DELETE-EAnnotation", "DELETE-EAttribute", "DELETE-EClass", "DELETE-EDataType", "DELETE-EEnum", "DELETE-EEnumLiteral",
				"DELETE-EGenericType", "DELETE-EOperation", "DELETE-EPackage", "DELETE-EReference", "DELETE-ETypeParameter",
				"DELETE-ResourceAttachment.EPackage", "MOVE-EAttribute", "MOVE-EClass", "MOVE-EDataType", "MOVE-EEnum", "MOVE-EEnumLiteral",
				"MOVE-EGenericType", "MOVE-EOperation", "MOVE-EPackage", "MOVE-EReference"]


discarded_features = [
                      # Discarded features related with generics: I've never used them myself
                      #   and I don't think they're that popular (in the Babur paper this
                      #   might be commented) -> check count_generics.py (not that much used)
                      "ADD-EGenericType", "ADD-ETypeParameter",
				              "CHANGE-EGenericType", "DELETE-EGenericType", "DELETE-ETypeParameter",
				              "MOVE-EGenericType", "CHANGE-EParameter",

                      # Resource attachments in the root that make no sense if you want to have
                      #   a well-formed metamodel tree (with an epackage in the root, subpackages, etc),
                      #   seem like "typos" or mistakes when defining a metamodel
                      "ADD-ResourceAttachment.EDataType"
                      ]

simple_features = ["ADD-EAnnotation", "ADD-EAttribute", "ADD-EClass", "ADD-EDataType", "ADD-EEnum", "ADD-EEnumLiteral",
                   "ADD-EOperation", "ADD-EPackage", "ADD-EReference",
                   "ADD-ResourceAttachment.EPackage", "CHANGE-EAnnotation", "CHANGE-EAttribute", "CHANGE-EClass", "CHANGE-EDataType", "CHANGE-EEnum",
                   "CHANGE-EEnumLiteral", "CHANGE-EOperation", "CHANGE-EPackage", "CHANGE-EReference",
                   "DELETE-EAnnotation", "DELETE-EAttribute", "DELETE-EClass", "DELETE-EDataType", "DELETE-EEnum", "DELETE-EEnumLiteral",
                   "DELETE-EOperation", "DELETE-EPackage", "DELETE-EReference",
                   "DELETE-ResourceAttachment.EPackage", "MOVE-EAttribute", "MOVE-EClass", "MOVE-EDataType", "MOVE-EEnum", "MOVE-EEnumLiteral",
                   "MOVE-EOperation", "MOVE-EPackage", "MOVE-EReference"]

# when grouping features, some of them are changed from "add" or "delete" to "change", just to reduce the
# detail of the features to the parent elements

# - In EAnnotations, any add or delete to their unbounded references counts as a "change annotation"
# - Same for EException and EParameters of an EOperation

grouped_features = {}

# Check no feature was left behind (by numbers):

real_num_features = len(all_features)
num_features = len(simple_features)
for name, group in grouped_features.items():
    num_features += len(group)

assert(real_num_features == (num_features + len(discarded_features)))

print("All features: ", real_num_features)
print("Reduced features: ", len(simple_features) + len(grouped_features.keys()))

base_columns = [c for c in data.columns if c not in all_features]
base_columns

# create grouped features
for name, group in grouped_features.items():
  data[name] = 0
  for feature in group:
    data[name] = data[name] + data[feature]

new_columns = base_columns + simple_features + list(grouped_features.keys())
new_columns

data = data[new_columns]

data_inter = data[data['repo_original'] != data['repo_duplicate']]
data_intra = data[data['repo_original'] == data['repo_duplicate']]

#%%

# unpaired wilcoxon test
from scipy.stats import mannwhitneyu

mannwhitneyu(data_intra['affected_elements'], data_inter['affected_elements'])

#%%
import numpy as np

print(f'Intra: {np.median(data_intra["affected_elements"])}')
print(f'Inter: {np.median(data_inter["affected_elements"])}')

#%%
# boxplot
import matplotlib.pyplot as plt

plt.boxplot([data_intra['affected_elements'], data_inter['affected_elements']])
# y log
plt.yscale('log')
plt.show()

#%%
"""# Cluster analysis"""

import xml.etree.ElementTree as ET

data = data[data['affected_elements']>0]
data

#%%

def count_xmi_id_attributes(xml_file):
    tree = ET.parse(xml_file)
    root = tree.getroot()
    count = 0

    for elem in root.iter():
        if '{http://www.omg.org/XMI}id' in elem.attrib:
            count += 1
    return count

def has_different_ids(row):
    original_ids = count_xmi_id_attributes(f"../metamodels/{row['original_path']}")
    duplicate_ids = count_xmi_id_attributes(f"../metamodels/{row['duplicate_path']}")

    return original_ids != duplicate_ids and (original_ids == 0 or duplicate_ids == 0)

def has_ids(row):
    original_ids = count_xmi_id_attributes(f"../metamodels/{row['original_path']}")
    duplicate_ids = count_xmi_id_attributes(f"../metamodels/{row['duplicate_path']}")
    result = 0

    if original_ids > 0:
        result += 1
    if duplicate_ids > 0:
        result += 1

    return result

# Uncomment to calculate

# data['different_ids'] = data.apply(has_different_ids, axis=1)

# has_ids: 0 -> no ids, 1 -> only one has ids, 2 -> both have ids
# data['has_ids'] = data.apply(has_ids, axis=1)
# data["has_ids"].value_counts()

#%%

# Extract list of metamodels used in comparison for other analyses

metamodels = set()
for index, row in data.iterrows():
    metamodels.add(row['original_path'])
    metamodels.add(row['duplicate_path'])

with open("metamodels.txt", "w") as f:
    for metamodel in metamodels:
        f.write(f"{metamodel}\n")


#%%
features = [c for c in data.columns if 'ADD' in c or 'CHANGE' in c or 'DELETE' in c or 'MOVE' in c]
len(features)

import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import normalize

X = data[features]
X = normalize(X, norm='l2')

pca = PCA(n_components=15)
X_transformed = pca.fit_transform(X)
np.sum(pca.explained_variance_ratio_)

#%%

import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# List to store the WCSS values
wcss = []
ks_considered = [2, 5, 10, 15, 20, 25, 30]

# Loop through different values of k (number of clusters)
for k in ks_considered:
    kmeans = KMeans(n_clusters=k, init='k-means++',
                    max_iter=300, n_init=10, random_state=0)
    kmeans.fit(X_transformed)
    wcss.append(kmeans.inertia_)  # Inertia is another term for WCSS

# Plotting the elbow plot
plt.figure(figsize=(8,5))
plt.plot(ks_considered, wcss, marker='o', linestyle='--')
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of clusters (k)')
plt.ylabel('WCSS')
plt.show()

#%%
from kneed import KneeLocator

kneedle = KneeLocator([2, 5, 10, 15, 20, 25, 30],
                      wcss, curve='convex', direction='decreasing')

elbow_point = kneedle.elbow
print(f"Elbow point: {elbow_point}")

kmeans = KMeans(n_clusters=kneedle.elbow, init='k-means++',
                    max_iter=300, n_init=10, random_state=0)
kmeans.fit(X_transformed)

cluster_sizes = np.bincount(kmeans.labels_)
cluster_sizes

for cluster in range(10):
    print(f"Cluster {cluster}: {cluster_sizes[cluster]}")
    data_cluster = data[kmeans.labels_ == cluster]
    data_cluster = pd.DataFrame(normalize(data_cluster[features], norm='l1'), columns=data_cluster[features].columns).mean()
    # top 10 features
    top_features = 100 * data_cluster.sort_values(ascending=False).head(10)
    print(top_features)
    print()

data['cluster_kmeans'] = kmeans.labels_


#%% take samples of kmeans clusters
'''
np.random.seed(43)
sample_size = 40
for cluster in range(10):
    data_cluster = data[kmeans.labels_ == cluster]
    data_cluster.sample(sample_size).to_csv(f"kmeans-cluster{cluster}-sample.csv", index=False)
'''

#%%
def group_clusters(label):
    if label in [2,3,8]: # estructurales (major?)
      return 0
    elif label in [0,5,9]: # anotaciones
      return 1
    elif label in [6,7]: # no estructurales (minor?)
      return 2
    elif label in [1,4]: # paquete
      return 3

data['cluster_kmeans_manual'] = data['cluster_kmeans'].apply(group_clusters)
data

#%%
data[data['cluster_kmeans_manual'] == 0].to_csv("macro_cluster0-structural-major.csv", index=False)
data[data['cluster_kmeans_manual'] == 1].to_csv("macro_cluster1-annotations.csv", index=False)
data[data['cluster_kmeans_manual'] == 2].to_csv("macro_cluster2-nonstructural-minor.csv", index=False)
data[data['cluster_kmeans_manual'] == 3].to_csv("macro_cluster3-package.csv", index=False)

#%%
data[data['inter']]['cluster_kmeans_manual'].hist(density=True)

#%%
data[~data['inter']]['cluster_kmeans_manual'].hist(density=True)

#%%
annotation_cluster = data[data['cluster_kmeans_manual'] == 1].copy()
annotation_cluster.shape

#%%
annotation_features = [c for c in features if 'EAnnotation' in c]
other_features = [c for c in features if c not in annotation_features]

print(annotation_features)
print(other_features)

# %%
annotation_cluster["annotation_changes"] = annotation_cluster[annotation_features].sum(axis=1)
annotation_cluster["other_changes"] = annotation_cluster[other_features].sum(axis=1)

annotation_cluster["changes_ratio"] = annotation_cluster["annotation_changes"] / (annotation_cluster["other_changes"] + annotation_cluster["annotation_changes"])

#%%
annotation_cluster["other_changes"].hist(bins=100)

# %%
annotation_cluster["annotation_changes"].agg(["median", "mean", "std", "min", "max"])

#%%
annotation_cluster["other_changes"].agg(["median", "mean", "std", "min", "max"])
# %%
annotation_cluster["changes_ratio"].agg(["median", "mean", "std", "min", "max"])

#%%
cumulative_distribution = annotation_cluster["changes_ratio"].value_counts().sort_index().cumsum()

# Plot the cumulative distribution
cumulative_distribution.plot()
plt.xlabel('Ratio of annotation changes')
plt.ylabel('Cumulative Frequency')
plt.title('Cumulative Distribution')
plt.show()

#%%
annotation_cluster.sort_values(["changes_ratio"]).to_csv("macro_cluster1-annotations-withratios.csv", index=False)
